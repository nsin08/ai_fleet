# Sprint 2026-W09

**Dates:** 2026-02-23 to 2026-03-01  
**Document Type:** Go-to-Market Phase 1 — Demo Stability + AI Grounding  
**Branch:** `feature/go-to-market-1`  
**Program Horizon:** W09 (Phase 1 of the go-to-market delivery track)

---

## Goals

- Close the 5 demo blockers identified in the market readiness assessment before the first customer-facing session.
- Ground AI responses in real fleet data (end hallucination, enable actionable answers).
- Rebrand and improve the AI assistant to "OpsEdge AI" — contextually-aware, data-grounded, named to match the FleetEdge brand.
- Add vehicle trail on the map for visual storytelling during replay demos.

---

## Product Outcomes (Business)

| Outcome | Current State | Target by W09 End |
|---|---|---|
| Demo stability | 5 show-stopping bugs visible in 30-min demo | Zero blockers — demo runs end-to-end cleanly |
| Dashboard accuracy | KPI strip shows 0 On Trip while map shows 14 moving vehicles | KPI matches map at all times |
| AI trustworthiness | AI answers with hallucinated driver IDs and made-up data | AI answers with real names, real vehicle IDs, real metrics from DB |
| Map storytelling | Dots teleport; no trail visible | Colored breadcrumb trail shows recent path per vehicle |
| Live emitter coverage | 1 of 14 vehicles emits in live mode | All 14 vehicles active in live mode |

---

## Background: Why AI Grounding is a Blocker

From the screenshot on 2026-02-20:

> User asked: *"who is top performing driver"*  
> OpsEdge AI responded: **"Driver ID 102 (Morning Shift)"** — a fully hallucinated answer.  
> Actual data: Driver table has names like Rajesh Patil, Amit Kumar, Vikram Joshi with real scores.

**Root cause:** The chat endpoint sends the user message with a context string (`page: /drivers`) but injects **no actual data**. The LLM answers from its training weights, not the database. In a customer demo, an AI that makes up employee names is worse than no AI.

**Fix required:** Before calling Ollama, the API route must:
1. Detect the semantic intent of the question
2. Fetch relevant live data from the database
3. Inject it as structured context into the system prompt
4. The model then answers grounded in that data

---

## Naming Decision: "OpsEdge AI"

**Rationale:** "Fleet Copilot" sounds generic and close to Microsoft Copilot branding. "OpsEdge AI" is:
- Aligned with the product name (FleetEdge)
- Signals operational focus (not a general assistant)
- Differentiates from generic AI helpers

**Changes required:**
- Rename "Fleet Copilot" → "OpsEdge AI" in the drawer header
- Update all prompt text references
- Update system prompts to say "OpsEdge AI" as the assistant persona

---

## Scope

### P0 In Scope (W09)

1. Dashboard KPI fix — "On Trip" reads from `vehicle_latest_state` not `vehicles`
2. Login page — demo credentials, session stored in localStorage
3. Live emitter scale-out — all 14 vehicles emitting in live mode
4. **AI graceful degradation** — Ollama adapter handles offline gracefully; no 500 errors
5. **OpsEdge AI grounding** — Hexagonal/SOLID: intent port → DB query adapter → context builder → Ollama adapter
6. OpsEdge AI rename — header, prompts, persona
7. Vehicle trail on map — breadcrumb polyline for last N positions

**Architecture Constraints (AI Work Only)**
- All AI calls route through `AiInferencePort` (hexagonal boundary)
- Ollama is the implementation adapter (swappable at `AiInferenceAdapter`)
- Intent detection and data fetching are separate concerns (separation of concerns)
- No direct HTTP calls to external LLMs; all go through port abstraction
- Context injection happens at domain layer, not at adapter layer

### P1 Deferred to W10

- Chart/sparkline visualizations
- CSV/PDF export
- Email/webhook alert notifications
- Live trip auto-creation on emitter transition from idle → on_trip
- Full authentication with session management

---

## Architecture

### AI Grounding Architecture (Hexagonal + SOLID)

**Ports & Adapters Structure:**
```
Domain Layer (Pure)
  ├─ AiUseCasePort (inbound) — chat, summarize, explain
  ├─ AiInferencePort (outbound) — abstract LLM interface
  ├─ IntentClassifierPort (outbound) — intent detection
  └─ RepositoryPort (outbound) — data fetching

Adapter Layer (I/O)
  ├─ Ollama adapter — implements AiInferencePort (uses deepseek-r1:8b)
  ├─ KeywordIntentAdapter — implements IntentClassifierPort
  └─ DbContextBuilder — queries fleet data, builds context JSON
```

**Current flow (Hallucination Problem):**
```
User message → /api/ai/chat → Ollama (no context) → Model invents answers
```

**Target flow (Grounded Responses):**
```
User message
  ↓
AiUseCase.chat() [domain]
  ↓ (via IntentClassifierPort)
detectIntent(message) — keyword matching
  ↓ (via RepositoryPort)
queryFleetData(intent) — DB SELECT, not LLM
  ↓ (pure function)
buildContextString(data) — format for system prompt
  ↓ (via AiInferencePort → Ollama adapter)
generateCompletion(systemPrompt + context + userMessage)
  ↓
Response grounded in real DB facts
```

**SOLID Principles Applied:**
- **S**ingle Responsibility: IntentClassifier ≠ ContextBuilder ≠ Ollama adapter
- **O**pen/Closed: New intents added without modifying Ollama adapter
- **L**iskov Substitution: Any LLM can replace Ollama if it implements AiInferencePort
- **I**nterface Segregation: Small focused ports (Intent ≠ Inference)
- **D**ependency Inversion: Controllers depend on ports, not concrete Ollama class

**Intent categories to support in W09:**

| Intent keyword | Data fetched | API query |
|---|---|---|
| `top driver`, `best driver`, `driver score`, `driver performance` | Top 5 drivers by score | `/api/drivers?orderBy=score&limit=5` |
| `available driver` | Available drivers list | `/api/drivers?availability=available` |
| `open alert`, `active alert`, `unresolved` | Open alert summary | `/api/alerts?status=OPEN&limit=20` |
| `on trip`, `active trip`, `vehicles moving` | Vehicle states snapshot | `/api/fleet/states` |
| `fuel anomaly`, `fuel alert` | Current fuel anomalies | `/api/fuel/anomalies?status=OPEN&limit=10` |
| `work order`, `maintenance due` | Open work orders | `/api/maintenance/work-orders?status=OPEN&limit=10` |
| `today summary`, `daily report`, `how are we doing` | Daily summary metrics | `/api/reports/daily` |

**Context injection format (injected as system message before user query):**
```
CURRENT FLEET DATA (queried live — use this, do not invent data):
{
  "asOf": "2026-02-20T10:30:00Z",
  "dataType": "top_drivers",
  "records": [
    { "id": "drv-mum-03", "name": "Rajesh Patil", "score": 90, "status": "on_trip", "risk": "LOW" },
    ...
  ]
}
Answer using only the data above. Do not invent driver names, vehicle IDs, or metrics.
```

### Map Trail Architecture

**Current:** Single `Marker` per vehicle state update (position overwrites previous)

**Target:** 
- Keep a `trailMap: Map<vehicleId, [lat, lng][]>` in React state (capped at last 30 points)
- On each `vehicleState` WebSocket message, append new position to trail
- Render `Polyline` per vehicle with matching status color at 40% opacity
- Trail fades on idle/parked status

**Component changes:** `fleet-map.tsx` receives `trails` prop alongside `states`

### Login Architecture (Demo Mode)

**Decision:** No real auth backend for W09. Demo login hardcoded to `admin / fleetedge2026` stored in localStorage with a `demo_session` flag.

**Reasoning:** Real OAuth adds 4–6 hours. Demo login adds visible legitimacy in 1 hour without blocking the demo.

**Future migration:** Replace localStorage check with JWT validation in W10.

---

## Implementation Work Packages

### WP-01 Dashboard KPI Accuracy

**User Story:**  
As an operator watching the dashboard, the "On Trip" KPI should always match the number of green dots on the map.

**Acceptance Criteria**
- At all times, "On Trip" count matches the number of `on_trip` status vehicles in `vehicle_latest_state`
- KPI updates within 5 seconds of status change
- Works in live mode with all 14 active emitters

**Technical Tasks**
- API: Change `GET /api/fleet/inventory` to join against `vehicle_latest_state.status` over `vehicles.status` (already done for some fields — verify On Trip is applying the join correctly)
- Web: Change `onTripCount` in `apps/web/src/app/page.tsx` to read from `states` array (already in memory) not `vehicles` array
- Verify in live mode: activate all 14 emitters, confirm "On Trip" reflects real moving vehicles

---

### WP-02 Login Page (Demo Mode)

**User Story:**  
As a customer evaluating the product, I see a professional login screen when I open the app, not an unrestricted interface.

**Acceptance Criteria**
- Opening `localhost:3000` when not logged in redirects to `/login`
- Login form accepts `admin / fleetedge2026` (hardcoded demo credentials)
- Login page shows the FleetEdge brand
- Incorrect credentials show an error message
- After login, user is redirected to dashboard
- Logout option in the nav

**Technical Tasks**
- Web: Create `apps/web/src/app/login/page.tsx` with brand and form
- Web: Add middleware (`apps/web/src/middleware.ts`) to check `demo_session` in cookie/localStorage and redirect to `/login` if absent
- Web (`nav-shell.tsx`): Add "Sign out" at bottom of sidebar, clears session and redirects to `/login`
- No backend changes required for W09

---

### WP-04 Live Emitter Scale-Out

**User Story:**  
As a demo operator, when switching to live mode, all 14 vehicles should appear and move on the map.

**Acceptance Criteria**
- In Docker Compose live profile, all 14 vehicle emitters start with correct `VEHICLE_ID` and `START_LAT/LNG`
- Each vehicle starts at a geographically unique position in the Delhi/Mumbai area
- All 14 appear on the map in live mode within 30 seconds of switching

**Technical Tasks**
- `docker-compose.yml`: Replace single `vehicle-emitter` service with 14 named services (`vehicle-emitter-veh-mum-01` → `veh-del-04`) each with correct env vars (VEHICLE_ID, VEHICLE_REG_NO, START_LAT, START_LNG)
- Starting positions: spread across the Delhi NCR grid (lat 28.4–29.2, lng 76.7–77.8)
- Profile: assign to `live` profile only (do not start during replay profile)
- Verify: run `docker compose --profile live up` and confirm 14 state rows appear in `/api/fleet/states` within 30s

---

### WP-05 AI Graceful Degradation

**User Story:**  
As an operator, if the AI service is temporarily unavailable, the rest of the app should continue working and show a friendly message instead of an error screen.

**Acceptance Criteria**
- If Ollama is down, `POST /api/ai/chat` returns HTTP 200 with `{ reply: "AI service is temporarily unavailable. Please try again.", model: "unavailable", evidence: {...} }`
- The alerts page "Explain Alert" button shows the same friendly message instead of a 500 error toast
- API logs the actual Ollama error at WARN level

**Technical Tasks**
- `ai.controller.ts`: Wrap all `ai.generateCompletion()` calls in try-catch; on error return structured fallback response
- `copilot-drawer.tsx`: No changes needed if API returns 200 with message
- `alerts/page.tsx`: Verify explain-alert error handling surfaces the message correctly

---

### WP-06 OpsEdge AI — Grounding + Rename

**User Story:**  
As an operator, when I ask OpsEdge AI "who is the top performing driver?", it responds with actual names and scores from the current fleet data — not made-up answers.

**Acceptance Criteria**
- Asking "who is top performing driver" returns real driver names and scores from `drivers` table
- Asking "which alerts are critical right now" returns actual open CRITICAL alerts from `alerts` table
- Asking "how many vehicles are on trip" returns actual count from `vehicle_latest_state`
- AI never invents a name, vehicle ID, or metric that doesn't exist in the data payload provided
- Drawer header shows "OpsEdge AI" (not "Fleet Copilot")
- Context badge shows module name (e.g., "Driver Ops")
- System prompt persona reads "You are OpsEdge AI, the fleet operations assistant for FleetEdge."

**Technical Tasks**

*Backend — `ai.controller.ts` and domain*
- Add `IntentClassifierPort` interface to `packages/domain/ports`
- Add `IntentClassifier` implementation (keyword matching) — implements port
- Add `buildGroundedContext(intent: IntentKey): Promise<ContextString>` in domain
  - Uses `RepositoryPort` to fetch real data (not HTTP calls)
  - Returns structured context string for system prompt injection
- Add graceful try-catch in `POST /api/ai/chat` around `inferencePort.generateCompletion()`
  - On error: return HTTP 200 with `{ reply: "AI service unavailable", model: "unavailable" }`
  - Log actual error at WARN level
- Update system prompt to inject `CURRENT_FLEET_DATA` block before user query
  - Format: `CURRENT_FLEET_DATA (live query result): { "type": "driver_performance", "records": [...] }`
  - Format: Add comment: `Answer using only this data. Do not invent names, IDs, or metrics.`

*Frontend — `copilot-drawer.tsx`*
- Rename "Fleet Copilot" → "OpsEdge AI" in header
- Update system prompt in drawer context if shown
- Verify error handling surfaces friendly message from API

---

### WP-07 Vehicle Trail on Map

**User Story:**  
As a demo viewer, I can see where each vehicle has been over the last few minutes as a colored breadcrumb trail on the map, making the replay feel alive and purposeful.

**Acceptance Criteria**
- During replay, each active vehicle shows a polyline of its last 30 positions
- Trail color matches vehicle status color (green for on_trip, blue for idle)
- Trail is 40% opacity so it doesn't obscure the map
- Trail clears when a vehicle goes idle/parked for more than 5 ticks
- Map does not lag or stutter with 14 active trails

**Technical Tasks**
- `page.tsx`: Add `trailsRef = useRef<Map<string, [number, number][]>>(new Map())` 
- `page.tsx`: In `vehicleState` WS handler, append `[lat, lng]` to trail for that vehicleId, cap at 30 entries
- `fleet-map.tsx`: Add `trails?: Map<string, [number, number][]>` prop
- `fleet-map.tsx`: For each vehicle in `visible`, render a `<Polyline>` with those trail points and color from `STATUS_COLOUR` at 40% opacity
- Verify: trails visible during `scenario-delhi-delivery` replay at 5x speed

---

## Sprint Delivery Plan

| Day | Focus | Exit Criteria |
|---|---|---|
| Mon Feb 23 | WP-01 (KPI) + WP-04 (AI degradation) | Dashboard KPI correct, AI never 500s |
| Tue Feb 24 | WP-02 (login) + WP-05 backend (AI grounding intent + data fetch) | Login page functional, `who is top driver` returns real data from DB |
| Wed Feb 25 | WP-05 frontend (rename + context injection) | OpsEdge AI live in drawer with grounded responses |
| Thu Feb 26 | WP-03 (emitter scale-out) + WP-06 (vehicle trail) | 14 emitters active in live mode, trails visible in replay |
| Fri Feb 27 | Integration, manual demo run, git commit, PR to develop | Full 30-min demo script runs without errors |

---

## Engineering Breakdown

### Backend

- `apps/api/src/controllers/ai.controller.ts`:
  - Add `detectIntent()` and `buildGroundedContext()` helper functions
  - Add graceful try-catch around all `generateCompletion()` calls
  - Inject grounded context as system message

### Frontend

- `apps/web/src/app/login/page.tsx` (new) — demo login form
- `apps/web/src/middleware.ts` (new) — session redirect guard
- `apps/web/src/app/page.tsx` — fix `onTripCount`, add trail state
- `apps/web/src/components/copilot-drawer.tsx` — rename to OpsEdge AI
- `apps/web/src/components/fleet-map.tsx` — add `trails` prop and `Polyline` rendering
- `apps/web/src/components/nav-shell.tsx` — add logout action

### DevOps / Config

- `docker-compose.yml` — expand vehicle-emitter to 14 services under `live` profile

---

## Definition of Done (Per Work Package)

- Product acceptance criteria validated
- No TypeScript compile errors (`npm run typecheck`)
- No lint errors (`npm run lint`)
- Manual happy-path verified against running Docker containers
- Existing 61 tests still passing (`npm test`)
- git committed on `feature/go-to-market-1`

---

## Risks and Mitigations

| Risk | Impact | Mitigation |
|---|---|---|
| Ollama context window too small for large data payloads | High | Cap context injection to 10 records max; summarize rather than dump raw SQL rows |
| `deepseek-r1:8b` ignores "do not invent data" instruction | High | Include data explicitly in user turn (not just system), use assertive grounding format |
| 14 emitters consume too much RAM in docker compose | Medium | Reduce `EMIT_INTERVAL_MS` to 5000ms in live profile; only start via `--profile live` |
| Login middleware breaks Next.js routing in docker | Medium | Test middleware with `NEXT_PUBLIC_DEMO_MODE=true` flag; fallback to simple client-side check |
| Trail polylines cause re-render on every WS tick | Medium | Use `useRef` for trail map, call `map.eachLayer` to update polylines imperatively instead of React state |

---

## Metrics (Target by W09 End)

- 0 show-stopper bugs reproducible in 30-min demo script
- AI returns real data for at least 5 of the 7 intent categories defined above
- All 14 vehicles visible in live mode within 30 seconds
- Vehicle trail visible within 15 seconds of starting a replay
- Login page loads and authenticates within 2 seconds

---

## Immediate Next Actions (W09 Start)

1. Fix `onTripCount` in `apps/web/src/app/page.tsx` — 30 min, immediate win
2. Add `detectIntent()` skeleton in `ai.controller.ts` with first 3 intent categories
3. Spike: test Ollama context injection with a hardcoded driver list and verify it stops hallucinating

---

## Success Criteria Checklist

> Update immediately after completing each item. Mark `[x]` with evidence.

### Global Gates

- [ ] All existing 61 tests still passing after each WP (no regressions)
- [ ] TypeScript compiles clean (`npm run typecheck`)
- [ ] Docker containers start and stay healthy
- [ ] Manual 30-min demo script completed without error

### WP-01 Dashboard KPI Accuracy

- [ ] `onTripCount` reads from `states` (vehicle_latest_state), not `vehicles`
- [ ] In live mode: "On Trip" KPI >= 1 within 10 seconds of emitter activation
- [ ] KPI matches number of green dots on map (manual count verified)

### WP-02 Login Page

- [ ] `/login` page renders with FleetEdge branding
- [ ] `admin / fleetedge2026` credentials accepted, redirects to dashboard
- [ ] Wrong credentials show error message
- [ ] Unauthenticated access to `/` redirects to `/login`
- [ ] Logout clears session and redirects to `/login`

### WP-04 Live Emitter Scale-Out

- [ ] `docker-compose.yml` contains 14 named emitter services
- [ ] Each has unique `VEHICLE_ID`, `VEHICLE_REG_NO`, `START_LAT`, `START_LNG`
- [ ] `docker compose --profile live up` starts all 14 without errors
- [ ] `/api/fleet/states` returns 14 rows within 30s of live mode activation

### WP-05 AI Graceful Degradation

- [ ] All `generateCompletion()` calls wrapped in try-catch in `ai.controller.ts`
- [ ] With Ollama container stopped: `POST /api/ai/chat` returns 200 with friendly message
- [ ] Alerts "Explain Alert" button shows message instead of crashing
- [ ] Error logged at WARN level in API container logs

### WP-06 OpsEdge AI — Grounding + Rename

- [ ] Drawer header shows "OpsEdge AI"
- [ ] `detectIntent()` function implemented with 5+ intent categories
- [ ] `buildGroundedContext()` queries DB and returns formatted string
- [ ] Intent: "who is top performing driver" → returns real names from DB
- [ ] Intent: "open alerts" → returns actual open alert count and top 5
- [ ] Intent: "vehicles on trip" → returns actual count from vehicle_latest_state
- [ ] System prompt uses "OpsEdge AI" persona
- [ ] Grounded context injected as system message before user query
- [ ] AI never mentions a name or ID not present in the injected data

### WP-07 Vehicle Trail on Map

- [ ] `fleet-map.tsx` accepts `trails` prop
- [ ] Trail polyline renders for each vehicle with positions
- [ ] Trail uses status color at 40% opacity
- [ ] Trail capped at 30 points per vehicle
- [ ] Trail visible during `scenario-delhi-delivery` at 5x speed
- [ ] No visible performance degradation with 14 active trails

### Sprint Exit

- [ ] All 6 WP checklists complete
- [ ] Feature branch merged to develop
- [ ] 30-min customer demo script run end-to-end with no crashes
- [ ] Market readiness moves from 65% → 85%+
