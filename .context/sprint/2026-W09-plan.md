# Sprint 2026-W09

**Dates:** 2026-02-23 to 2026-03-01  
**Document Type:** Go-to-Market Phase 1 — Demo Stability + AI Grounding  
**Branch:** `feature/go-to-market-1`  
**Program Horizon:** W09 (Phase 1 of the go-to-market delivery track)

---

## Goals

- Close the 5 demo blockers identified in the market readiness assessment before the first customer-facing session.
- Ground AI responses in real fleet data (end hallucination, enable actionable answers).
- Rebrand and improve the AI assistant to "OpsEdge AI" — contextually-aware, data-grounded, named to match the FleetEdge brand.
- Add vehicle trail on the map for visual storytelling during replay demos.

---

## Product Outcomes (Business)

| Outcome | Current State | Target by W09 End |
|---|---|---|
| Demo stability | 5 show-stopping bugs visible in 30-min demo | Zero blockers — demo runs end-to-end cleanly |
| Dashboard accuracy | KPI strip shows 0 On Trip while map shows 14 moving vehicles | KPI matches map at all times |
| AI trustworthiness | AI answers with hallucinated driver IDs and made-up data | AI answers with real names, real vehicle IDs, real metrics from DB |
| Map storytelling | Dots teleport; no trail visible | Colored breadcrumb trail shows recent path per vehicle |
| Live emitter coverage | 1 of 14 vehicles emits in live mode | All 14 vehicles active in live mode |

---

## Background: Why AI Grounding is a Blocker

From the screenshot on 2026-02-20:

> User asked: *"who is top performing driver"*  
> OpsEdge AI responded: **"Driver ID 102 (Morning Shift)"** — a fully hallucinated answer.  
> Actual data: Driver table has names like Rajesh Patil, Amit Kumar, Vikram Joshi with real scores.

**Root cause:** The chat endpoint sends the user message with a context string (`page: /drivers`) but injects **no actual data**. The LLM answers from its training weights, not the database. In a customer demo, an AI that makes up employee names is worse than no AI.

**Fix required:** Before calling Ollama, the API route must:
1. Detect the semantic intent of the question
2. Fetch relevant live data from the database
3. Inject it as structured context into the system prompt
4. The model then answers grounded in that data

---

## Naming Decision: "OpsEdge AI"

**Rationale:** "Fleet Copilot" sounds generic and close to Microsoft Copilot branding. "OpsEdge AI" is:
- Aligned with the product name (FleetEdge)
- Signals operational focus (not a general assistant)
- Differentiates from generic AI helpers

**Changes required:**
- Rename "Fleet Copilot" → "OpsEdge AI" in the drawer header
- Update all prompt text references
- Update system prompts to say "OpsEdge AI" as the assistant persona

---

## Scope

### P0 In Scope (W09)

1. Dashboard KPI fix — "On Trip" reads from `vehicle_latest_state` not `vehicles`
2. Login page — demo credentials, session stored in localStorage
3. Live emitter scale-out — all 14 vehicles emitting in live mode
4. **AI graceful degradation** — LangChain-wrapped adapter handles model unavailability gracefully; no 500 errors
5. **OpsEdge AI grounding** — LangGraph state machine with intent detection, context fetching, and multi-turn reasoning
6. OpsEdge AI rename — header, prompts, persona
7. Vehicle trail on map — breadcrumb polyline for last N positions

**Architecture Constraints (AI Work Only)**
- Use **LangChain** for model abstraction and tool use
- Use **LangGraph** for stateful agentic reasoning (intent → data fetch → reasoning → response)
- **Configurable models:** Ollama (local), OpenAI (gpt-4), Claude (claude-3-5-sonnet via Anthropic)
- Model selection via `AI_PROVIDER` env var: `ollama | openai | claude`
- Intent detection and data fetching are separate tools in the agent
- No direct LLM calls; all routed through LangChain `BaseLanguageModel` abstraction

### P1 Deferred to W10

- Chart/sparkline visualizations
- CSV/PDF export
- Email/webhook alert notifications
- Live trip auto-creation on emitter transition from idle → on_trip
- Full authentication with session management

---

## Architecture

### AI Grounding Architecture (LangGraph + Multi-Provider)

**LangGraph State Machine:**
```
┌─ ClassifyIntent Node
│  └─ Extract semantic intent from user message (tools: detect_driver_intent, detect_alert_intent, etc.)
│
├─ FetchContextData Node
│  └─ Call appropriate repository method based on intent (tools: get_driver_performance, get_open_alerts, etc.)
│
├─ ReasoningNode (Multi-turn)
│  └─ Agent reasons about data, asks clarifying questions if needed
│  └─ Loop back to FetchContextData if more data needed (e.g., "top driver in which region?")
│
└─ GenerateResponse Node
   └─ Format final response with evidence references
```

**Provider Abstraction (LangChain BaseLanguageModel):**
```
AI_PROVIDER=ollama     → ChatOllama(model="deepseek-r1:8b", base_url="http://localhost:11434")
AI_PROVIDER=openai     → ChatOpenAI(model="gpt-4", api_key=env.OPENAI_API_KEY)
AI_PROVIDER=claude     → ChatAnthropic(model="claude-3-5-sonnet-20241022", api_key=env.ANTHROPIC_API_KEY)
```

**Node Architecture:**
```
Domain Layer (Pure)
  ├─ Fleet Data Tools (repositories wrapped as LangChain tools)
  ├─ Intent Classifier Tool (keyword + semantic detection)
  └─ Response Formatter (pure function)

LangGraph StatefulAgent Layer
  ├─ State schema: { messages: BaseMessage[], intent: str, context: dict, tool_calls: list }
  ├─ Nodes: ClassifyIntent → FetchContext → Reasoning → GenerateResponse
  └─ Tool bindings: All DB queries as structured tools

LangChain Adapter Layer
  ├─ ChatOllama (local, no API key)
  ├─ ChatOpenAI (cloud, requires OPENAI_API_KEY)
  └─ ChatAnthropic (cloud, requires ANTHROPIC_API_KEY)
```

**Current Flow Problem:**
```
User message → Direct Ollama → No context → Hallucinated response
```

**Target Flow (Agentic Reasoning):**
```
User message
  ↓
LangGraph Agent (agentic loop)
  ├─ Node: ClassifyIntent
  │  └─ "who is top driver" → detect_driver_intent(query="top driver")
  │
  ├─ Node: FetchContextData
  │  └─ Tool call: get_top_drivers(limit=5) → SELECT * FROM drivers ORDER BY score DESC LIMIT 5
  │
  ├─ Node: Reasoning
  │  └─ Agent reasons: "Based on the data: Rajesh Patil (score: 90), Amit (87)..."
  │  └─ Can ask clarifying questions ("In which depot?") and loop back for more data
  │
  └─ Node: GenerateResponse
     └─ Formatted response with real data + evidence
```

**Multi-Turn Reasoning Example:**
```
User: "Who is top driver?"
Agent reason: "Need to know filter criteria. Should I consider all depots or specific one?"
Agent tool call: suggest_clarifications()
Response: "I found top 3 drivers overall: Rajesh Patil (All depots, score 90)..."

User: "In Mumbai depot only?"
Agent reason: "Re-filter by depot"
Agent tool call: get_top_drivers(depot="mum") 
Response: "In Mumbai: Vikram Joshi (88), Priya Singh (85)..."
```

**Intent categories to support in W09:**

| Intent keyword | Data fetched | API query |
|---|---|---|
| `top driver`, `best driver`, `driver score`, `driver performance` | Top 5 drivers by score | `/api/drivers?orderBy=score&limit=5` |
| `available driver` | Available drivers list | `/api/drivers?availability=available` |
| `open alert`, `active alert`, `unresolved` | Open alert summary | `/api/alerts?status=OPEN&limit=20` |
| `on trip`, `active trip`, `vehicles moving` | Vehicle states snapshot | `/api/fleet/states` |
| `fuel anomaly`, `fuel alert` | Current fuel anomalies | `/api/fuel/anomalies?status=OPEN&limit=10` |
| `work order`, `maintenance due` | Open work orders | `/api/maintenance/work-orders?status=OPEN&limit=10` |
| `today summary`, `daily report`, `how are we doing` | Daily summary metrics | `/api/reports/daily` |

**Context injection format (injected as system message before user query):**
```
CURRENT FLEET DATA (queried live — use this, do not invent data):
{
  "asOf": "2026-02-20T10:30:00Z",
  "dataType": "top_drivers",
  "records": [
    { "id": "drv-mum-03", "name": "Rajesh Patil", "score": 90, "status": "on_trip", "risk": "LOW" },
    ...
  ]
}
Answer using only the data above. Do not invent driver names, vehicle IDs, or metrics.
```

### Map Trail Architecture

**Current:** Single `Marker` per vehicle state update (position overwrites previous)

**Target:** 
- Keep a `trailMap: Map<vehicleId, [lat, lng][]>` in React state (capped at last 30 points)
- On each `vehicleState` WebSocket message, append new position to trail
- Render `Polyline` per vehicle with matching status color at 40% opacity
- Trail fades on idle/parked status

**Component changes:** `fleet-map.tsx` receives `trails` prop alongside `states`

### Login Architecture (Demo Mode)

**Decision:** No real auth backend for W09. Demo login hardcoded to `admin / fleetedge2026` stored in localStorage with a `demo_session` flag.

**Reasoning:** Real OAuth adds 4–6 hours. Demo login adds visible legitimacy in 1 hour without blocking the demo.

**Future migration:** Replace localStorage check with JWT validation in W10.

---

## Implementation Work Packages

### WP-01 Dashboard KPI Accuracy

**User Story:**  
As an operator watching the dashboard, the "On Trip" KPI should always match the number of green dots on the map.

**Acceptance Criteria**
- At all times, "On Trip" count matches the number of `on_trip` status vehicles in `vehicle_latest_state`
- KPI updates within 5 seconds of status change
- Works in live mode with all 14 active emitters

**Technical Tasks**
- API: Change `GET /api/fleet/inventory` to join against `vehicle_latest_state.status` over `vehicles.status` (already done for some fields — verify On Trip is applying the join correctly)
- Web: Change `onTripCount` in `apps/web/src/app/page.tsx` to read from `states` array (already in memory) not `vehicles` array
- Verify in live mode: activate all 14 emitters, confirm "On Trip" reflects real moving vehicles

---

### WP-02 Login Page (Demo Mode)

**User Story:**  
As a customer evaluating the product, I see a professional login screen when I open the app, not an unrestricted interface.

**Acceptance Criteria**
- Opening `localhost:3000` when not logged in redirects to `/login`
- Login form accepts `admin / fleetedge2026` (hardcoded demo credentials)
- Login page shows the FleetEdge brand
- Incorrect credentials show an error message
- After login, user is redirected to dashboard
- Logout option in the nav

**Technical Tasks**
- Web: Create `apps/web/src/app/login/page.tsx` with brand and form
- Web: Add middleware (`apps/web/src/middleware.ts`) to check `demo_session` in cookie/localStorage and redirect to `/login` if absent
- Web (`nav-shell.tsx`): Add "Sign out" at bottom of sidebar, clears session and redirects to `/login`
- No backend changes required for W09

---

### WP-04 Live Emitter Scale-Out

**User Story:**  
As a demo operator, when switching to live mode, all 14 vehicles should appear and move on the map.

**Acceptance Criteria**
- In Docker Compose live profile, all 14 vehicle emitters start with correct `VEHICLE_ID` and `START_LAT/LNG`
- Each vehicle starts at a geographically unique position in the Delhi/Mumbai area
- All 14 appear on the map in live mode within 30 seconds of switching

**Technical Tasks**
- `docker-compose.yml`: Replace single `vehicle-emitter` service with 14 named services (`vehicle-emitter-veh-mum-01` → `veh-del-04`) each with correct env vars (VEHICLE_ID, VEHICLE_REG_NO, START_LAT, START_LNG)
- Starting positions: spread across the Delhi NCR grid (lat 28.4–29.2, lng 76.7–77.8)
- Profile: assign to `live` profile only (do not start during replay profile)
- Verify: run `docker compose --profile live up` and confirm 14 state rows appear in `/api/fleet/states` within 30s

---

### WP-05 AI Graceful Degradation

**User Story:**  
As an operator, if the AI service is temporarily unavailable, the rest of the app should continue working and show a friendly message instead of an error screen.

**Acceptance Criteria**
- If Ollama is down, `POST /api/ai/chat` returns HTTP 200 with `{ reply: "AI service is temporarily unavailable. Please try again.", model: "unavailable", evidence: {...} }`
- The alerts page "Explain Alert" button shows the same friendly message instead of a 500 error toast
- API logs the actual Ollama error at WARN level

**Technical Tasks**
- `ai.controller.ts`: Wrap all `ai.generateCompletion()` calls in try-catch; on error return structured fallback response
- `copilot-drawer.tsx`: No changes needed if API returns 200 with message
- `alerts/page.tsx`: Verify explain-alert error handling surfaces the message correctly

---

### WP-06 OpsEdge AI — Grounding + Rename

**User Story:**  
As an operator, when I ask OpsEdge AI "who is the top performing driver?", it responds with actual names and scores from the current fleet data — not made-up answers. The AI supports multi-turn reasoning and can ask clarifying questions.

**Acceptance Criteria**
- Asking "who is top performing driver" returns real driver names and scores from `drivers` table (no hallucinations)
- Asking "which alerts are critical right now" returns actual open CRITICAL alerts from `alerts` table
- Asking "give me a summary" triggers multi-turn reasoning (agent may ask clarifying questions)
- Model is configurable via `AI_PROVIDER` env var (`ollama`, `openai`, `claude`)
- When Ollama/OpenAI/Claude is unavailable, `/api/ai/chat` returns HTTP 200 with friendly message (no 500 errors)
- Drawer header shows "OpsEdge AI" 
- System logs show reasoning steps and tool calls for debugging

**Environment Variables (W09)**
```env
# Model provider (default: ollama)
AI_PROVIDER=ollama           # or: openai, claude

# Ollama (local, no key required)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_CHAT_MODEL=deepseek-r1:8b

# OpenAI (if AI_PROVIDER=openai)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4

# Claude (if AI_PROVIDER=claude)
ANTHROPIC_API_KEY=sk-ant-...
CLAUDE_MODEL=claude-3-5-sonnet-20241022
```

**Technical Tasks**

*Backend Dependencies (`package.json`)*
- Add `langchain`, `langgraph`, and provider clients:
  ```
  npm install langchain @langchain/core @langchain/community
  npm install langgraph @langgraph/prebuilt
  npm install @langchain/anthropic  # Claude support
  npm install @langchain/openai     # OpenAI support  (optional if using Ollama only)
  ```

*Backend — `apps/api/src/services/ai/`* (new directory structure)
```
ai/
├── state-schema.ts           # LangGraph state definition
├── tools/
│   ├── fetch-driver-data.ts  # DB tools exported as LangChain tools
│   ├── fetch-alert-data.ts
│   ├── fetch-trip-data.ts
│   └── index.ts              # register all tools
├── nodes/
│   ├── classify-intent.ts    # Node: detect intent from user message
│   ├── fetch-context.ts      # Node: call appropriate DB tool
│   ├── reasoning.ts          # Node: agent reasoning loop
│   └── generate-response.ts  # Node: format response
├── agent.ts                  # LangGraph graph construction + compile()
└── index.ts                  # export agent factory
```

*Backend — `apps/api/src/config/ai-provider.ts`* (NEW)
```typescript
export function initAiModel(): BaseLanguageModel {
  const provider = process.env.AI_PROVIDER || 'ollama';
  
  switch(provider) {
    case 'openai':
      return new ChatOpenAI({
        modelName: 'gpt-4',
        apiKey: process.env.OPENAI_API_KEY,
      });
    case 'claude':
      return new ChatAnthropic({
        modelName: 'claude-3-5-sonnet-20241022',
        apiKey: process.env.ANTHROPIC_API_KEY,
      });
    case 'ollama':
    default:
      return new ChatOllama({
        baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
        model: 'deepseek-r1:8b',
      });
  }
}
```

*Backend — `apps/api/src/controllers/ai.controller.ts`*
- Import compiled agent from `ai/agent.ts`
- `POST /api/ai/chat` handler:
  ```typescript
  const state = {
    messages: [new HumanMessage(userMessage)],
    intent: null,
    context: {},
    toolCalls: []
  };
  
  const result = await agent.invoke(state, { configurable: { userId } });
  return { reply: result.messages[-1].content, model: 'opsedge-ai' };
  ```
- Wrap in try-catch; on error return HTTP 200 with friendly message (no 500)

*Frontend — `copilot-drawer.tsx`*
- Rename "Fleet Copilot" → "OpsEdge AI" in header
- Display reasoning steps if agent provides them (optional: show "thinking" animation)
- Error handling: display friendly message from API

**Test Scenarios**
- [ ] Ollama online: ask "top driver", receive real names + scores
- [ ] Ollama offline: ask "top driver", receive friendly fallback message (HTTP 200)
- [ ] Multi-turn: ask "top driver", agent asks "all depots?", user says "Mumbai only", agent re-fetches and refines answer
- [ ] Switch `AI_PROVIDER=openai` in env, restart, same prompts work with GPT-4
- [ ] Switch `AI_PROVIDER=claude`, restart, same prompts work with Claude

---

### WP-07 Vehicle Trail on Map

**User Story:**  
As a demo viewer, I can see where each vehicle has been over the last few minutes as a colored breadcrumb trail on the map, making the replay feel alive and purposeful.

**Acceptance Criteria**
- During replay, each active vehicle shows a polyline of its last 30 positions
- Trail color matches vehicle status color (green for on_trip, blue for idle)
- Trail is 40% opacity so it doesn't obscure the map
- Trail clears when a vehicle goes idle/parked for more than 5 ticks
- Map does not lag or stutter with 14 active trails

**Technical Tasks**
- `page.tsx`: Add `trailsRef = useRef<Map<string, [number, number][]>>(new Map())` 
- `page.tsx`: In `vehicleState` WS handler, append `[lat, lng]` to trail for that vehicleId, cap at 30 entries
- `fleet-map.tsx`: Add `trails?: Map<string, [number, number][]>` prop
- `fleet-map.tsx`: For each vehicle in `visible`, render a `<Polyline>` with those trail points and color from `STATUS_COLOUR` at 40% opacity
- Verify: trails visible during `scenario-delhi-delivery` replay at 5x speed

---

## Sprint Delivery Plan

| Day | Focus | Exit Criteria |
|---|---|---|
| Mon Feb 23 | WP-01 (KPI) + WP-04 (AI degradation) | Dashboard KPI correct, AI never 500s |
| Tue Feb 24 | WP-02 (login) + WP-05 backend (AI grounding intent + data fetch) | Login page functional, `who is top driver` returns real data from DB |
| Wed Feb 25 | WP-05 frontend (rename + context injection) | OpsEdge AI live in drawer with grounded responses |
| Thu Feb 26 | WP-03 (emitter scale-out) + WP-06 (vehicle trail) | 14 emitters active in live mode, trails visible in replay |
| Fri Feb 27 | Integration, manual demo run, git commit, PR to develop | Full 30-min demo script runs without errors |

---

## Engineering Breakdown

### Backend

- `apps/api/src/controllers/ai.controller.ts`:
  - Add `detectIntent()` and `buildGroundedContext()` helper functions
  - Add graceful try-catch around all `generateCompletion()` calls
  - Inject grounded context as system message

### Frontend

- `apps/web/src/app/login/page.tsx` (new) — demo login form
- `apps/web/src/middleware.ts` (new) — session redirect guard
- `apps/web/src/app/page.tsx` — fix `onTripCount`, add trail state
- `apps/web/src/components/copilot-drawer.tsx` — rename to OpsEdge AI
- `apps/web/src/components/fleet-map.tsx` — add `trails` prop and `Polyline` rendering
- `apps/web/src/components/nav-shell.tsx` — add logout action

### DevOps / Config

- `docker-compose.yml` — expand vehicle-emitter to 14 services under `live` profile

---

## Definition of Done (Per Work Package)

- Product acceptance criteria validated
- No TypeScript compile errors (`npm run typecheck`)
- No lint errors (`npm run lint`)
- Manual happy-path verified against running Docker containers
- Existing 61 tests still passing (`npm test`)
- git committed on `feature/go-to-market-1`

---

## Risks and Mitigations

| Risk | Impact | Mitigation |
|---|---|---|
| LangGraph complexity in W09 timeline | High | Start with single-tool agent (driver lookup only), test end-to-end before adding more tools |
| Model provider switching requires restart | Medium | Could implement runtime provider switching in W10; hardcode for W09 |
| Reasoning loop gets stuck (infinite tool calls) | Medium | Set max_iterations=10 in agent; log all steps for debugging |
| OpenAI/Claude API costs | Medium | W09 uses free Ollama only; optional keys for testing; no auto-billing |
| LangChain version mismatches with TypeScript | High | Lock versions in package.json; test with `npm run typecheck` after install |

---

## Metrics (Target by W09 End)

- 0 show-stopper bugs reproducible in 30-min demo script
- LangGraph agent successfully routes 5+ intent types to correct tools and returns real DB data
- 3+ model providers (Ollama, OpenAI, Claude) can be toggled via `AI_PROVIDER` env var
- All 14 vehicles visible in live mode within 30 seconds
- Vehicle trail visible within 15 seconds of starting a replay
- Login page loads and authenticates within 2 seconds
- Multi-turn reasoning demonstrated (agent asks clarifying question, user refines, agent re-fetches and refines answer)

---

## Immediate Next Actions (W09 Start)

1. Fix `onTripCount` in `apps/web/src/app/page.tsx` — 30 min, immediate win
2. Install LangChain/LangGraph packages and spike single-tool agent (driver lookup) — verify basic agentic loop works
3. Implement first LangGraph node (ClassifyIntent) with keyword matching for 3 intent types

---

## Success Criteria Checklist

> Update immediately after completing each item. Mark `[x]` with evidence.

### Global Gates

- [ ] All existing 61 tests still passing after each WP (no regressions)
- [ ] TypeScript compiles clean (`npm run typecheck`)
- [ ] Docker containers start and stay healthy
- [ ] Manual 30-min demo script completed without error

### WP-01 Dashboard KPI Accuracy

- [ ] `onTripCount` reads from `states` (vehicle_latest_state), not `vehicles`
- [ ] In live mode: "On Trip" KPI >= 1 within 10 seconds of emitter activation
- [ ] KPI matches number of green dots on map (manual count verified)

### WP-02 Login Page

- [ ] `/login` page renders with FleetEdge branding
- [ ] `admin / fleetedge2026` credentials accepted, redirects to dashboard
- [ ] Wrong credentials show error message
- [ ] Unauthenticated access to `/` redirects to `/login`
- [ ] Logout clears session and redirects to `/login`

### WP-04 Live Emitter Scale-Out

- [ ] `docker-compose.yml` contains 14 named emitter services
- [ ] Each has unique `VEHICLE_ID`, `VEHICLE_REG_NO`, `START_LAT`, `START_LNG`
- [ ] `docker compose --profile live up` starts all 14 without errors
- [ ] `/api/fleet/states` returns 14 rows within 30s of live mode activation

### WP-05 AI Graceful Degradation

- [ ] All `generateCompletion()` calls wrapped in try-catch in `ai.controller.ts`
- [ ] With Ollama container stopped: `POST /api/ai/chat` returns 200 with friendly message
- [ ] Alerts "Explain Alert" button shows message instead of crashing
- [ ] Error logged at WARN level in API container logs

### WP-06 OpsEdge AI — Grounding + Rename

- [ ] Drawer header shows "OpsEdge AI"
- [ ] LangChain packages installed (langchain, @langchain/community, langgraph)
- [ ] `apps/api/src/config/ai-provider.ts` created with multi-provider factory
- [ ] LangGraph agent with ClassifyIntent, FetchContext, Reasoning, GenerateResponse nodes implemented
- [ ] All DB tools (driver, alert, trip) registered as LangChain tools
- [ ] `AI_PROVIDER=ollama` (default) works with deepseek-r1:8b locally
- [ ] `AI_PROVIDER=openai` with OPENAI_API_KEY works with GPT-4
- [ ] `AI_PROVIDER=claude` with ANTHROPIC_API_KEY works with Claude
- [ ] Intent: "who is top performing driver" → returns real names from `drivers` table
- [ ] Intent: "open alerts" → returns actual open CRITICAL alerts from `alerts` table
- [ ] Multi-turn: "top driver" → agent asks "all depots or specific?", refines on follow-up
- [ ] System prompt injects `CURRENT_FLEET_DATA` JSON before user query
- [ ] AI never mentions a name, ID, or metric not present in injected data
- [ ] With model service down (Ollama stopped, OPENAI_API_KEY missing, etc.): `POST /api/ai/chat` returns HTTP 200 with friendly message
- [ ] Reasoning steps logged to API debug output
- [ ] Frontend receives and displays agent responses without error

### WP-07 Vehicle Trail on Map

- [ ] `fleet-map.tsx` accepts `trails` prop
- [ ] Trail polyline renders for each vehicle with positions
- [ ] Trail uses status color at 40% opacity
- [ ] Trail capped at 30 points per vehicle
- [ ] Trail visible during `scenario-delhi-delivery` at 5x speed
- [ ] No visible performance degradation with 14 active trails

### Sprint Exit

- [ ] All 6 WP checklists complete
- [ ] Feature branch merged to develop
- [ ] 30-min customer demo script run end-to-end with no crashes
- [ ] Market readiness moves from 65% → 85%+
