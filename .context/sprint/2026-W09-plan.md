# Sprint 2026-W09

**Dates:** 2026-02-23 to 2026-03-01  
**Document Type:** Go-to-Market Phase 1 — Demo Stability + AI Grounding  
**Branch:** `feature/go-to-market-1`  
**Program Horizon:** W09 (Phase 1 of the go-to-market delivery track)

---

## Goals

- Close the 5 demo blockers identified in the market readiness assessment before the first customer-facing session.
- Ground AI responses in real fleet data (end hallucination, enable actionable answers).
- Rebrand and improve the AI assistant to "OpsEdge AI" — contextually-aware, data-grounded, named to match the FleetEdge brand.
- Add vehicle trail on the map for visual storytelling during replay demos.

---

## Product Outcomes (Business)

| Outcome | Current State | Target by W09 End |
|---|---|---|
| Demo stability | 5 show-stopping bugs visible in 30-min demo | Zero blockers — demo runs end-to-end cleanly |
| Dashboard accuracy | KPI strip shows 0 On Trip while map shows 14 moving vehicles | KPI matches map at all times |
| AI trustworthiness | AI answers with hallucinated driver IDs and made-up data | AI answers with real names, real vehicle IDs, real metrics from DB |
| Map storytelling | Dots teleport; no trail visible | Colored breadcrumb trail shows recent path per vehicle |
| Live emitter coverage | 1 of 14 vehicles emits in live mode | All 14 vehicles active in live mode |

---

## Background: Why AI Grounding is a Blocker

From the screenshot on 2026-02-20:

> User asked: *"who is top performing driver"*  
> OpsEdge AI responded: **"Driver ID 102 (Morning Shift)"** — a fully hallucinated answer.  
> Actual data: Driver table has names like Rajesh Patil, Amit Kumar, Vikram Joshi with real scores.

**Root cause:** The chat endpoint sends the user message with a context string (`page: /drivers`) but injects **no actual data**. The LLM answers from its training weights, not the database. In a customer demo, an AI that makes up employee names is worse than no AI.

**Fix required:** Before calling Ollama, the API route must:
1. Detect the semantic intent of the question
2. Fetch relevant live data from the database
3. Inject it as structured context into the system prompt
4. The model then answers grounded in that data

---

## Naming Decision: "OpsEdge AI"

**Rationale:** "Fleet Copilot" sounds generic and close to Microsoft Copilot branding. "OpsEdge AI" is:
- Aligned with the product name (FleetEdge)
- Signals operational focus (not a general assistant)
- Differentiates from generic AI helpers

**Changes required:**
- Rename "Fleet Copilot" → "OpsEdge AI" in the drawer header
- Update all prompt text references
- Update system prompts to say "OpsEdge AI" as the assistant persona

---

## Scope

### P0 In Scope (W09)

1. Dashboard KPI fix — "On Trip" reads from `vehicle_latest_state` not `vehicles`
2. Login page — demo credentials, session stored in localStorage
3. Live emitter scale-out — all 14 vehicles emitting in live mode
4. **AI graceful degradation** — LangChain-wrapped adapter handles model unavailability gracefully; no 500 errors
5. **OpsEdge AI grounding** — LangGraph state machine with intent detection, context fetching via Neo4j graph vector store, and multi-turn reasoning
6. **Neo4j Graph Vector Store** — Replace traditional vector embedding with Neo4j for semantic search on fleet data, driver relationships, alert correlations, and trip patterns
7. OpsEdge AI rename — header, prompts, persona
8. Vehicle trail on map — breadcrumb polyline for last N positions

**Architecture Constraints (AI Work Only)**
- Use **LangChain** for model abstraction and tool use
- Use **LangGraph** for stateful agentic reasoning (intent → data fetch → reasoning → response)
- **Configurable models:** Ollama (local), OpenAI (gpt-4), Claude (claude-3-5-sonnet via Anthropic)
- Model selection via `AI_PROVIDER` env var: `ollama | openai | claude`
- Intent detection and data fetching are separate tools in the agent
- No direct LLM calls; all routed through LangChain `BaseLanguageModel` abstraction

### P1 Deferred to W10

- Chart/sparkline visualizations
- CSV/PDF export
- Email/webhook alert notifications
- Live trip auto-creation on emitter transition from idle → on_trip
- Full authentication with session management
- Advanced graph queries (multi-hop relationships, pattern detection)
- Graph ML embeddings and similarity scoring

---

## Architecture

### AI Grounding Architecture (LangGraph + Neo4j Graph Vector + Multi-Provider)

**Neo4j Graph Vector Store:**
- Fleet entities as nodes: `(Driver)`, `(Vehicle)`, `(Trip)`, `(Alert)`, `(Depot)`
- Relationships: `DRIVES`, `ASSIGNED_TO`, `CREATED_ALERT`, `LOCATED_IN`, `COMPLETED_TRIP`
- Vector embeddings for semantic search on driver/vehicle descriptions, alert summaries, trip notes
- Similarity search: "Find drivers with similar score patterns to top performer"
- Relationship queries: "Which drivers completed trips from Depot A?"
- Aggregation: Graph native path ranking for correlation analysis

**LangGraph State Machine (with Neo4j context):**
```
┌─ ClassifyIntent Node
│  └─ Extract semantic intent from user message (tools: detect_driver_intent, detect_alert_intent, etc.)
│
├─ FetchContextData Node (Neo4j-backed)
│  ├─ Keyword search: Direct node property lookup (driver name, vehicle ID)
│  ├─ Semantic search: Vector similarity on embeddings (similar performance drivers, anomaly patterns)
│  └─ Relationship traversal: Multi-hop queries ("drivers in region → associated vehicles → recent trips")
│
├─ ReasoningNode (Multi-turn)
│  └─ Agent reasons about data + relationships, asks clarifying questions if needed
│  └─ Loop back to FetchContextData if more context needed
│
└─ GenerateResponse Node
   └─ Format final response with evidence references and relationship context
```

**Provider Abstraction (LangChain BaseLanguageModel):**
```
AI_PROVIDER=ollama     → ChatOllama(model="deepseek-r1:8b", base_url="http://localhost:11434")
AI_PROVIDER=openai     → ChatOpenAI(model="gpt-4", api_key=env.OPENAI_API_KEY)
AI_PROVIDER=claude     → ChatAnthropic(model="claude-3-5-sonnet-20241022", api_key=env.ANTHROPIC_API_KEY)

Graph Vector Store (Neo4j):
  NEO4J_URI=bolt://localhost:7687
  NEO4J_USER=neo4j
  NEO4J_PASSWORD=password
  EMBEDDING_MODEL=mxbai-embed-large (via Ollama)
```

**Node Architecture:**
```
Domain Layer (Pure)
  ├─ Fleet Data Tools (repositories wrapped as LangChain tools)
  ├─ Intent Classifier Tool (keyword + semantic detection)
  └─ Response Formatter (pure function)

LangGraph StatefulAgent Layer
  ├─ State schema: { messages: BaseMessage[], intent: str, context: dict, tool_calls: list }
  ├─ Nodes: ClassifyIntent → FetchContext → Reasoning → GenerateResponse
  └─ Tool bindings: All DB queries as structured tools

LangChain Adapter Layer
  ├─ ChatOllama (local, no API key)
  ├─ ChatOpenAI (cloud, requires OPENAI_API_KEY)
  └─ ChatAnthropic (cloud, requires ANTHROPIC_API_KEY)
```

**Current Flow Problem:**
```
User message → Direct Ollama → No context → Hallucinated response
```

**Target Flow (Agentic Reasoning with Neo4j Graph Vector):**
```
User message
  ↓
LangGraph Agent (agentic loop)
  ├─ Node: ClassifyIntent
  │  └─ "who is top driver" → detect_driver_intent(query="top driver")
  │
  ├─ Node: FetchContextData (Neo4j-backed)
  │  ├─ Keyword search: MATCH (d:Driver) WHERE d.name =~ '.*rajesh.*' RETURN d
  │  ├─ Semantic search vector: MATCH (d:Driver) WHERE similarity(d.embedding, query_embedding) > 0.8 RETURN d
  │  ├─ Relationship traversal: MATCH (d:Driver)-[:ASSIGNED_TO]->(v:Vehicle)-[:LOCATED_IN]->(dep:Depot) WHERE dep.name='Mumbai' RETURN d, v, dep
  │  └─ Tool calls: neo4j_keyword_search(), neo4j_semantic_search(), neo4j_relationship_query()
  │
  ├─ Node: Reasoning
  │  └─ Agent reasons: "Based on graph data: Rajesh Patil (score: 90, drives vehicle VEH-MUM-01 in Mumbai depot)..."
  │  └─ Can ask clarifying questions ("In which depot?") and loop back for more Neo4j queries
  │
  └─ Node: GenerateResponse
     └─ Formatted response with real data + relationship context + evidence
```

**Multi-Turn Reasoning Example (with Neo4j):**
```
User: "Who is top driver?"
Agent reason: "Need to know filter criteria. Should I consider all depots or specific one?"
Agent tool call: neo4j_keyword_search(query="top driver")
Response: "I found top 3 drivers overall: Rajesh Patil (All depots, score 90, drives VEH-MUM-01)..."

User: "In Mumbai depot only?"
Agent reason: "Filter by depot using graph relationship"
Agent tool call: neo4j_relationship_query(query="MATCH (d:Driver)-[:LOCATED_IN]->(dep:Depot) WHERE dep.name='Mumbai' ORDER BY d.score DESC")
Response: "In Mumbai: Vikram Joshi (88, VEH-MUM-03), Priya Singh (85, VEH-MUM-02)..."

User: "Who drives similar to Rajesh?"
Agent reason: "Use semantic similarity on driver embeddings"
Agent tool call: neo4j_semantic_search(query="find drivers with similar patterns to Rajesh", embedding=rajesh_embedding)
Response: "Similar drivers: Amit Kumar (score 87, VEH-MUM-04 - similar highway metrics)..."
```

**Intent categories to support in W09 (with Neo4j queries):**

| Intent keyword | Neo4j Query | LangChain Tool |
|---|---|---|
| `top driver`, `best driver`, `driver score`, `driver performance` | `MATCH (d:Driver) ORDER BY d.score DESC LIMIT 5` | `neo4j_keyword_search()` |
| `available driver` | `MATCH (d:Driver {status: 'available'}) RETURN d` | `neo4j_keyword_search()` |
| `open alert`, `active alert`, `unresolved` | `MATCH (a:Alert {status: 'OPEN'}) RETURN a LIMIT 20` | `neo4j_keyword_search()` or `neo4j_semantic_search()` (for alert type matching) |
| `on trip`, `active trip`, `vehicles moving` | `MATCH (v:Vehicle {status: 'on_trip'}) RETURN v` | `neo4j_keyword_search()` |
| `fuel anomaly`, `fuel alert` | `MATCH (a:Alert)-[:RELATED_TO]->(v:Vehicle) WHERE a.type='FUEL' AND a.status='OPEN' RETURN a, v` | `neo4j_relationship_query()` |
| `work order`, `maintenance due` | `MATCH (wo:WorkOrder {status: 'OPEN'})-[:ASSIGNED_TO]->(v:Vehicle) RETURN wo, v` | `neo4j_relationship_query()` |
| `similar to [driver]` | `MATCH (d1:Driver {name:'Rajesh'}) RETURN drivers_by_similarity(d1, 0.8)` | `neo4j_semantic_search()` |
| `today summary`, `daily report` | `MATCH (t:Trip) WHERE t.date=today() MATCHING (t)-[:COMPLETED_BY]->(d:Driver) RETURN count(t), avg(t.duration)` | `neo4j_relationship_query()` |

**Context injection format (injected as system message before user query — from Neo4j):**
```json
CURRENT FLEET DATA (queried live from Neo4j graph — use this data only, do not invent):
{
  "asOf": "2026-02-20T10:30:00Z",
  "queryType": "keyword_lookup_with_relationships",
  "nodes": [
    {
      "type": "Driver",
      "id": "drv-mum-03",
      "name": "Rajesh Patil",
      "score": 90,
      "status": "on_trip",
      "risk": "LOW"
    }
  ],
  "relationships": [
    {
      "source": "drv-mum-03",
      "type": "DRIVES",
      "target": "veh-mum-01",
      "properties": {"since": "2025-06-15", "trips_completed": 248}
    },
    {
      "source": "veh-mum-01",
      "type": "LOCATED_IN",
      "target": "depot-mum",
      "properties": {"region": "Mumbai"}
    }
  ],
  "semanticMatches": [
    {"entity": "drv-mum-03", "similarity": 0.92, "reason": "Similar driving pattern to query"}
  ]
}

Answer using ONLY the data above. Reference node IDs and relationships when explaining.
Do not invent driver names, vehicle IDs, trip counts, or metrics not in the graph data.
Include relationship context in your response (e.g., "Rajesh (drv-mum-03) DRIVES vehicle veh-mum-01").
```

### Map Trail Architecture

**Current:** Single `Marker` per vehicle state update (position overwrites previous)

**Target:** 
- Keep a `trailMap: Map<vehicleId, [lat, lng][]>` in React state (capped at last 30 points)
- On each `vehicleState` WebSocket message, append new position to trail
- Render `Polyline` per vehicle with matching status color at 40% opacity
- Trail fades on idle/parked status

**Component changes:** `fleet-map.tsx` receives `trails` prop alongside `states`

### Login Architecture (Demo Mode)

**Decision:** No real auth backend for W09. Demo login hardcoded to `admin / fleetedge2026` stored in localStorage with a `demo_session` flag.

**Reasoning:** Real OAuth adds 4–6 hours. Demo login adds visible legitimacy in 1 hour without blocking the demo.

**Future migration:** Replace localStorage check with JWT validation in W10.

---

## Implementation Work Packages

### WP-01 Dashboard KPI Accuracy

**User Story:**  
As an operator watching the dashboard, the "On Trip" KPI should always match the number of green dots on the map.

**Acceptance Criteria**
- At all times, "On Trip" count matches the number of `on_trip` status vehicles in `vehicle_latest_state`
- KPI updates within 5 seconds of status change
- Works in live mode with all 14 active emitters

**Technical Tasks**
- API: Change `GET /api/fleet/inventory` to join against `vehicle_latest_state.status` over `vehicles.status` (already done for some fields — verify On Trip is applying the join correctly)
- Web: Change `onTripCount` in `apps/web/src/app/page.tsx` to read from `states` array (already in memory) not `vehicles` array
- Verify in live mode: activate all 14 emitters, confirm "On Trip" reflects real moving vehicles

---

### WP-02 Login Page (Demo Mode)

**User Story:**  
As a customer evaluating the product, I see a professional login screen when I open the app, not an unrestricted interface.

**Acceptance Criteria**
- Opening `localhost:3000` when not logged in redirects to `/login`
- Login form accepts `admin / fleetedge2026` (hardcoded demo credentials)
- Login page shows the FleetEdge brand
- Incorrect credentials show an error message
- After login, user is redirected to dashboard
- Logout option in the nav

**Technical Tasks**
- Web: Create `apps/web/src/app/login/page.tsx` with brand and form
- Web: Add middleware (`apps/web/src/middleware.ts`) to check `demo_session` in cookie/localStorage and redirect to `/login` if absent
- Web (`nav-shell.tsx`): Add "Sign out" at bottom of sidebar, clears session and redirects to `/login`
- No backend changes required for W09

---

### WP-03 Live Emitter Scale-Out

**User Story:**  
As a demo operator, when switching to live mode, all 14 vehicles should appear and move on the map.

**Acceptance Criteria**
- In Docker Compose live profile, all 14 vehicle emitters start with correct `VEHICLE_ID` and `START_LAT/LNG`
- Each vehicle starts at a geographically unique position in the Delhi/Mumbai area
- All 14 appear on the map in live mode within 30 seconds of switching

**Technical Tasks**
- `docker-compose.yml`: Replace single `vehicle-emitter` service with 14 named services (`vehicle-emitter-veh-mum-01` → `veh-del-04`) each with correct env vars (VEHICLE_ID, VEHICLE_REG_NO, START_LAT, START_LNG)
- Starting positions: spread across the Delhi NCR grid (lat 28.4–29.2, lng 76.7–77.8)
- Profile: assign to `live` profile only (do not start during replay profile)
- Verify: run `docker compose --profile live up` and confirm 14 state rows appear in `/api/fleet/states` within 30s

---

### WP-04 AI Graceful Degradation

**User Story:**  
As an operator, if the AI service is temporarily unavailable, the rest of the app should continue working and show a friendly message instead of an error screen.

**Acceptance Criteria**
- If Ollama is down, `POST /api/ai/chat` returns HTTP 200 with `{ reply: "AI service is temporarily unavailable. Please try again.", model: "unavailable", evidence: {...} }`
- The alerts page "Explain Alert" button shows the same friendly message instead of a 500 error toast
- API logs the actual Ollama error at WARN level

**Technical Tasks**
- `ai.controller.ts`: Wrap all `ai.generateCompletion()` calls in try-catch; on error return structured fallback response
- `copilot-drawer.tsx`: No changes needed if API returns 200 with message
- `alerts/page.tsx`: Verify explain-alert error handling surfaces the message correctly

---

### WP-05 OpsEdge AI — Grounding + Rename

**User Story:**  
As an operator, when I ask OpsEdge AI "who is the top performing driver?", it responds with actual names and scores from the current fleet data — not made-up answers. The AI supports multi-turn reasoning and can ask clarifying questions.

**Acceptance Criteria**
- Asking "who is top performing driver" returns real driver names and scores from `drivers` table (no hallucinations)
- Asking "which alerts are critical right now" returns actual open CRITICAL alerts from `alerts` table
- Asking "give me a summary" triggers multi-turn reasoning (agent may ask clarifying questions)
- Model is configurable via `AI_PROVIDER` env var (`ollama`, `openai`, `claude`)
- When Ollama/OpenAI/Claude is unavailable, `/api/ai/chat` returns HTTP 200 with friendly message (no 500 errors)
- Drawer header shows "OpsEdge AI" 
- System logs show reasoning steps and tool calls for debugging

**Environment Variables (W09 — with Neo4j)**
```env
# Model provider (default: ollama)
AI_PROVIDER=ollama           # or: openai, claude

# Ollama (local, no key required)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_CHAT_MODEL=deepseek-r1:8b
OLLAMA_EMBED_MODEL=mxbai-embed-large:latest

# Neo4j Graph Vector Store (REQUIRED for graph context)
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# OpenAI (if AI_PROVIDER=openai)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4

# Claude (if AI_PROVIDER=claude)
ANTHROPIC_API_KEY=sk-ant-...
CLAUDE_MODEL=claude-3-5-sonnet-20241022
```

**Technical Tasks**

*Backend Dependencies (`package.json`)*
- Add `langchain`, `langgraph`, and provider clients:
  ```
  npm install langchain @langchain/core @langchain/community
  npm install langgraph @langgraph/prebuilt
  npm install @langchain/anthropic  # Claude support
  npm install @langchain/openai     # OpenAI support  (optional if using Ollama only)
  ```

*Backend — `apps/api/src/services/ai/`* (new directory structure)
```
ai/
├── state-schema.ts           # LangGraph state definition
├── tools/
│   ├── fetch-driver-data.ts  # DB tools exported as LangChain tools
│   ├── fetch-alert-data.ts
│   ├── fetch-trip-data.ts
│   └── index.ts              # register all tools
├── nodes/
│   ├── classify-intent.ts    # Node: detect intent from user message
│   ├── fetch-context.ts      # Node: call appropriate DB tool
│   ├── reasoning.ts          # Node: agent reasoning loop
│   └── generate-response.ts  # Node: format response
├── agent.ts                  # LangGraph graph construction + compile()
└── index.ts                  # export agent factory
```

*Backend — `apps/api/src/config/ai-provider.ts`* (NEW)
```typescript
export function initAiModel(): BaseLanguageModel {
  const provider = process.env.AI_PROVIDER || 'ollama';
  
  switch(provider) {
    case 'openai':
      return new ChatOpenAI({
        modelName: 'gpt-4',
        apiKey: process.env.OPENAI_API_KEY,
      });
    case 'claude':
      return new ChatAnthropic({
        modelName: 'claude-3-5-sonnet-20241022',
        apiKey: process.env.ANTHROPIC_API_KEY,
      });
    case 'ollama':
    default:
      return new ChatOllama({
        baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
        model: 'deepseek-r1:8b',
      });
  }
}
```

*Backend — `apps/api/src/controllers/ai.controller.ts`*
- Import compiled agent from `ai/agent.ts`
- `POST /api/ai/chat` handler:
  ```typescript
  const state = {
    messages: [new HumanMessage(userMessage)],
    intent: null,
    context: {},
    toolCalls: []
  };
  
  const result = await agent.invoke(state, { configurable: { userId } });
  return { reply: result.messages[-1].content, model: 'opsedge-ai' };
  ```
- Wrap in try-catch; on error return HTTP 200 with friendly message (no 500)

*Frontend — `copilot-drawer.tsx`*
- Rename "Fleet Copilot" → "OpsEdge AI" in header
- Display reasoning steps if agent provides them (optional: show "thinking" animation)
- Error handling: display friendly message from API

**Test Scenarios**
- [ ] Ollama online: ask "top driver", receive real names + scores
- [ ] Ollama offline: ask "top driver", receive friendly fallback message (HTTP 200)
- [ ] Multi-turn: ask "top driver", agent asks "all depots?", user says "Mumbai only", agent re-fetches and refines answer
- [ ] Switch `AI_PROVIDER=openai` in env, restart, same prompts work with GPT-4
- [ ] Switch `AI_PROVIDER=claude`, restart, same prompts work with Claude

---

### WP-06 Vehicle Trail on Map

**User Story:**  
As a demo viewer, I can see where each vehicle has been over the last few minutes as a colored breadcrumb trail on the map, making the replay feel alive and purposeful.

**Acceptance Criteria**
- During replay, each active vehicle shows a polyline of its last 30 positions
- Trail color matches vehicle status color (green for on_trip, blue for idle)
- Trail is 40% opacity so it doesn't obscure the map
- Trail clears when a vehicle goes idle/parked for more than 5 ticks
- Map does not lag or stutter with 14 active trails

**Technical Tasks**
- `page.tsx`: Add `trailsRef = useRef<Map<string, [number, number][]>>(new Map())` 
- `page.tsx`: In `vehicleState` WS handler, append `[lat, lng]` to trail for that vehicleId, cap at 30 entries
- `fleet-map.tsx`: Add `trails?: Map<string, [number, number][]>` prop
- `fleet-map.tsx`: For each vehicle in `visible`, render a `<Polyline>` with those trail points and color from `STATUS_COLOUR` at 40% opacity
- Verify: trails visible during `scenario-delhi-delivery` replay at 5x speed

---

## Sprint Delivery Plan

| Day | Focus | Exit Criteria |
|---|---|---|
| Mon Feb 23 | WP-01 (KPI) + WP-04 (AI degradation) | Dashboard KPI correct, AI never 500s |
| Tue Feb 24 | WP-02 (login) + Neo4j setup | Login page functional, Neo4j schema initialized, PostgreSQL → Neo4j sync working |
| Wed Feb 25 | WP-05 backend (intent + Neo4j tools) | `who is top driver` returns real data from Neo4j, semantic search working |
| Thu Feb 26 | WP-05 frontend (rename) + WP-03 (emitter scale-out) + WP-06 (vehicle trail) | OpsEdge AI live in drawer, 14 emitters active, trails visible |
| Fri Feb 27 | Integration, embedding generation, manual demo run, PR to develop | Full 30-min demo script runs without errors, Neo4j graph synced and searchable |

---

## Engineering Breakdown

### Backend — Neo4j & AI Grounding

- `apps/api/src/config/ai-provider.ts` (new):
  - `initAiModel()` — factory for ChatOllama / ChatOpenAI / ChatAnthropic based on `AI_PROVIDER`
  - `initNeo4jDriver()` — Neo4j driver with connection pooling

- `apps/api/src/services/neo4j/` (new directory):
  - `neo4j.client.ts` — driver initialization, health checks
  - `neo4j-sync.service.ts` — periodic sync from PostgreSQL → Neo4j; syncs drivers, vehicles, alerts, work orders, relationships
  - `embedding-generator.ts` — generates embeddings for driver descriptions and alert summaries using Ollama; batches updates to Neo4j

- `apps/api/src/services/ai/` (expanded for LangGraph + Neo4j):
  - `state-schema.ts` — LangGraph state: messages, intent, context, Neo4j transaction handle
  - `tools/neo4j-keyword-search.ts` — direct property-based search on nodes
  - `tools/neo4j-semantic-search.ts` — vector similarity search on embeddings
  - `tools/neo4j-relationship-query.ts` — multi-hop relationship traversal (safe, pre-validated patterns)
  - `tools/neo4j-aggregation-query.ts` — summary queries and reports
  - `tools/index.ts` — register all tools with LangChain
  - `nodes/classify-intent.ts` — detect intent from user query
  - `nodes/fetch-context.ts` — dispatch to appropriate Neo4j tool based on intent
  - `nodes/reasoning.ts` — agent multi-turn reasoning loop
  - `nodes/generate-response.ts` — format response with evidence and relationships
  - `agent.ts` — LangGraph graph construction with node connections + compile()
  - `index.ts` — export agent factory

- `db/neo4j/schema.cypher` (new):
  - Node types: Driver, Vehicle, Alert, Depot, Trip, WorkOrder
  - Constraints: unique IDs
  - Vector indices: driver_embedding_index, alert_embedding_index
  - Relationship types: DRIVES, ASSIGNED_TO, CREATED_ALERT, LOCATED_IN, COMPLETED_TRIP

- `apps/api/src/controllers/ai.controller.ts` (updated):
  - Import compiled agent from `ai/agent.ts`
  - `POST /api/ai/chat` — invoke LangGraph agent with user message
  - Try-catch for graceful degradation (return HTTP 200 with friendly message on error)
  - Log Neo4j queries at DEBUG level for transparency

- `apps/api/src/app.ts` (startup sequence):
  - Initialize Neo4j driver and verify connectivity
  - Load and execute schema.cypher (idempotent)
  - Run initial PostgreSQL → Neo4j sync
  - Schedule periodic sync every 5 minutes
  - Initialize embedding generator

### Frontend — Login & OpsEdge AI

- `apps/web/src/app/login/page.tsx` (new) — demo login form
- `apps/web/src/middleware.ts` (new) — session redirect guard
- `apps/web/src/app/page.tsx` — fix `onTripCount`, add trail state
- `apps/web/src/components/copilot-drawer.tsx` — rename to OpsEdge AI, display reasoning steps and Neo4j query summaries
- `apps/web/src/components/fleet-map.tsx` — add `trails` prop and `Polyline` rendering
- `apps/web/src/components/nav-shell.tsx` — add logout action

### DevOps / Config

- `docker-compose.yml`:
  - Add `neo4j` service (5.15-community) with Bolt (7687) and Browser (7474) ports
  - Add `neo4j_data` volume
  - Expand `vehicle-emitter` to 14 named services under `live` profile
  - Link all services via `ai-fleet` network
  - Add health checks for Neo4j

- `.env.example` (updated):
  - `NEO4J_URI=bolt://neo4j:7687`
  - `NEO4J_USER=neo4j`
  - `NEO4J_PASSWORD=password`
  - `AI_PROVIDER=ollama | openai | claude`
  - `OLLAMA_BASE_URL`, `OLLAMA_CHAT_MODEL`, etc.

---

## Definition of Done (Per Work Package)

- Product acceptance criteria validated
- No TypeScript compile errors (`npm run typecheck`)
- No lint errors (`npm run lint`)
- Manual happy-path verified against running Docker containers
- Existing 61 tests still passing (`npm test`)
- git committed on `feature/go-to-market-1`

---

## Risks and Mitigations

| Risk | Impact | Mitigation |
|---|---|---|
| Neo4j schema complexity and initial sync | High | Start with minimal schema (drivers, vehicles, basic relationships); test sync in isolation before integrating with AI |
| Vector embedding generation is slow (large batch) | Medium | Generate embeddings incrementally; cache results; use batch API if Ollama supports it |
| Neo4j-LangChain integration maturity | Medium | Validate library versions before W09 start; spike simple query on day 1 |
| LangGraph complexity in W09 timeline | High | Start with single-tool agent (driver lookup only), test end-to-end before adding more tools |
| Model provider switching requires restart | Medium | Could implement runtime provider switching in W10; hardcode for W09 |
| Reasoning loop gets stuck (infinite tool calls) | Medium | Set max_iterations=10 in agent; log all steps for debugging; pre-validate Cypher queries |
| OpenAI/Claude API costs | Medium | W09 uses free Ollama only; optional keys for testing; no auto-billing |
| LangChain version mismatches with TypeScript | High | Lock versions in package.json; test with `npm run typecheck` after install |
| PostgreSQL → Neo4j sync conflicts (missing data, duplicates) | Medium | Implement idempotent sync logic (upsert with unique constraints); monitor sync logs; manual verify first sync |
| Neo4j disk space (Docker volume) | Low | Monitor volume size; add `volumes.driver` settings in docker-compose; warn if >5GB |

---

## Metrics (Target by W09 End)

- 0 show-stopper bugs reproducible in 30-min demo script
- LangGraph agent with Neo4j tools successfully routes 5+ intent types and returns real fleet data
- 3+ model providers (Ollama, OpenAI, Claude) can be toggled via `AI_PROVIDER` env var
- Neo4j graph contains 100% of drivers, vehicles, alerts, and work orders from PostgreSQL (verified by count queries)
- Vector embeddings generated and indexed for all driver descriptions and alert summaries (>95 nodes embedded)
- Semantic search returns relevant results within 500ms for 8+ intent types
- Relationship queries (e.g., "drivers in region") execute in <100ms
- PostgreSQL ↔ Neo4j sync runs every 5 minutes with <2 second delta (new records visible in Neo4j within 7 minutes)
- All 14 vehicles visible in live mode within 30 seconds
- Vehicle trail visible within 15 seconds of starting a replay
- Login page loads and authenticates within 2 seconds
- Multi-turn reasoning demonstrated (agent asks clarifying question, user refines, agent re-fetches from Neo4j and refines answer)
- AI responses never hallucinate: 100% of mentioned entities (driver names, vehicle IDs, metrics) are verifiable in Neo4j graph data

---

## Immediate Next Actions (W09 Start)

1. Fix `onTripCount` in `apps/web/src/app/page.tsx` — 30 min, immediate win
2. Install LangChain/LangGraph packages and spike single-tool agent (driver lookup) — verify basic agentic loop works
3. Implement first LangGraph node (ClassifyIntent) with keyword matching for 3 intent types

---

## Success Criteria Checklist

> Update immediately after completing each item. Mark `[x]` with evidence.

### Global Gates

- [ ] All existing 61 tests still passing after each WP (no regressions)
- [ ] TypeScript compiles clean (`npm run typecheck`)
- [ ] Docker containers start and stay healthy
- [ ] Manual 30-min demo script completed without error

### WP-01 Dashboard KPI Accuracy

- [ ] `onTripCount` reads from `states` (vehicle_latest_state), not `vehicles`
- [ ] In live mode: "On Trip" KPI >= 1 within 10 seconds of emitter activation
- [ ] KPI matches number of green dots on map (manual count verified)

### WP-02 Login Page

- [ ] `/login` page renders with FleetEdge branding
- [ ] `admin / fleetedge2026` credentials accepted, redirects to dashboard
- [ ] Wrong credentials show error message
- [ ] Unauthenticated access to `/` redirects to `/login`
- [ ] Logout clears session and redirects to `/login`

### WP-03 Live Emitter Scale-Out

- [ ] `docker-compose.yml` contains 14 named emitter services
- [ ] Each has unique `VEHICLE_ID`, `VEHICLE_REG_NO`, `START_LAT`, `START_LNG`
- [ ] `docker compose --profile live up` starts all 14 without errors
- [ ] `/api/fleet/states` returns 14 rows within 30s of live mode activation

### WP-04 AI Graceful Degradation

- [ ] All `generateCompletion()` calls wrapped in try-catch in `ai.controller.ts`
- [ ] With Ollama container stopped: `POST /api/ai/chat` returns 200 with friendly message
- [ ] Alerts "Explain Alert" button shows message instead of crashing
- [ ] Error logged at WARN level in API container logs

### WP-05 OpsEdge AI — Grounding + Neo4j Graph Vector + Rename

**Neo4j Setup & Sync**
- [ ] `docker-compose.yml` Neo4j service added with Bolt (7687) and Browser (7474) exposed
- [ ] Neo4j Browser accessible at `http://localhost:7474`
- [ ] Neo4j authentication configured: `neo4j/password`
- [ ] `db/neo4j/schema.cypher` created with node types, constraints, and vector indices
- [ ] Neo4j schema loads successfully on app startup (idempotent)
- [ ] PostgreSQL → Neo4j sync service implemented and tested in isolation
- [ ] Initial sync completes within 5 seconds; all drivers/vehicles/alerts present in Neo4j
- [ ] Periodic sync runs every 5 minutes with no errors
- [ ] New driver added to PostgreSQL appears in Neo4j within 7 minutes (sync + embedding lag)
- [ ] Neo4j data counts match PostgreSQL counts (drivers, vehicles, alerts, work orders)

**Vector Embeddings**
- [ ] Ollama embedding model `mxbai-embed-large` available and working
- [ ] Embedding generator creates 1024-dimensional vectors for all driver descriptions
- [ ] Embedding generator creates 1024-dimensional vectors for all alert summaries
- [ ] Vector indices created: `driver_embedding_index`, `alert_embedding_index`
- [ ] Embeddings synced to Neo4j nodes (d.embedding, a.embedding properties)
- [ ] Semantic search queries execute in <500ms

**LangChain & LangGraph Setup**
- [ ] Drawer header shows "OpsEdge AI"
- [ ] LangChain packages installed (langchain, @langchain/core, @langchain/community, langgraph, @langchain/neo4j)
- [ ] `apps/api/src/config/ai-provider.ts` created with multi-provider factory
- [ ] `apps/api/src/services/neo4j/` directory created with: neo4j.client.ts, neo4j-sync.service.ts, embedding-generator.ts
- [ ] LangGraph agent with ClassifyIntent, FetchContext, Reasoning, GenerateResponse nodes implemented
- [ ] Neo4j tools registered as LangChain tools: neo4j_keyword_search, neo4j_semantic_search, neo4j_relationship_query, neo4j_aggregation_query
- [ ] `AI_PROVIDER=ollama` (default) works with deepseek-r1:8b locally backed by Neo4j
- [ ] `AI_PROVIDER=openai` with OPENAI_API_KEY works with GPT-4 backed by Neo4j
- [ ] `AI_PROVIDER=claude` with ANTHROPIC_API_KEY works with Claude backed by Neo4j

**Functional Tests**
- [ ] Intent: "who is top performing driver" → returns real names + scores from Neo4j (no hallucination)
- [ ] Intent: "which drivers in Mumbai depot" → traverses Neo4j relationships and returns filtered list
- [ ] Intent: "find drivers similar to Rajesh" → semantic search via embeddings returns relevant results
- [ ] Intent: "open alerts" → returns actual open CRITICAL alerts from Neo4j
- [ ] Intent: "trip summary" → aggregation query returns correct counts and metrics
- [ ] Multi-turn: "top driver" → agent asks "all depots or specific?", user says "Mumbai", agent re-queries Neo4j
- [ ] Responses reference node IDs and relationships (e.g., "Rajesh (drv-mum-03) DRIVES veh-mum-01")
- [ ] AI never mentions a name, ID, or metric not in Neo4j graph data
- [ ] With Neo4j stopped: `POST /api/ai/chat` returns HTTP 200 with friendly message (no 500)
- [ ] With Ollama stopped: `POST /api/ai/chat` returns HTTP 200 with friendly message
- [ ] Reasoning steps logged to API debug output (visible in docker logs)
- [ ] System prompt injects Neo4j query results and relationship context
- [ ] Frontend displays agent responses with evidence (node IDs, relationships) without error
- [ ] Neo4j queries visible in API debug logs (Cypher, execution time, result count)

### WP-06 Vehicle Trail on Map

- [ ] `fleet-map.tsx` accepts `trails` prop
- [ ] Trail polyline renders for each vehicle with positions
- [ ] Trail uses status color at 40% opacity
- [ ] Trail capped at 30 points per vehicle
- [ ] Trail visible during `scenario-delhi-delivery` at 5x speed
- [ ] No visible performance degradation with 14 active trails

### Sprint Exit

- [ ] All 6 WP checklists complete
- [ ] Feature branch merged to develop
- [ ] 30-min customer demo script run end-to-end with no crashes
- [ ] Market readiness moves from 65% → 85%+
